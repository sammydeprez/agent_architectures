{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea669370",
   "metadata": {},
   "source": [
    "# 2.1 Using Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f6edd",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Tool calling (also known as function calling) extends basic LLM capabilities by allowing the model to execute external functions. This pattern enables:\n",
    "\n",
    "- **External Data Access**: Retrieve real-time information (weather, stock prices, etc.)\n",
    "- **Action Execution**: Perform operations beyond text generation\n",
    "- **Structured Outputs**: Get formatted responses through function parameters\n",
    "\n",
    "### Key Concept\n",
    "\n",
    "**Important**: The LLM doesn't actually execute tools itself. Instead:\n",
    "1. The LLM **analyzes the request** and determines which tool to use\n",
    "2. The LLM **returns structured instructions** about what tool to call\n",
    "3. **Your application** executes the tool and gets the result\n",
    "4. **Your application** sends the tool result back to the LLM\n",
    "5. The LLM **formulates a final response** using the tool output\n",
    "\n",
    "This requires **two separate LLM calls**: one to determine the tool, and another to process the result.\n",
    "\n",
    "## Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa434df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolCall, ToolMessage\n",
    "from typing import Literal\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621924ca",
   "metadata": {},
   "source": [
    "### Step 1: Initialize the Language Model\n",
    "\n",
    "Same as before, we start with our base LLM connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a0b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0c4f0",
   "metadata": {},
   "source": [
    "### Step 2: Define Tools/Functions\n",
    "\n",
    "We create a tool using the `@tool` decorator. This tool:\n",
    "- **Has a clear docstring** describing its purpose and parameters\n",
    "- **Uses type hints** to specify allowed input values\n",
    "- **Returns structured data** that the LLM can interpret\n",
    "\n",
    "The `Literal` type restricts inputs to specific cities, making the tool more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53a8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def weather_tool(location: Literal[\"Chicago\", \"New York\", \"Los Angeles\"]) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a specified location.\n",
    "    Args:\n",
    "        location (str): The name of the city to get the weather for. \n",
    "                        Must be one of \"Chicago\", \"New York\", or \"Los Angeles\".\n",
    "    Returns:\n",
    "        str: A string describing the current weather in the specified location.\n",
    "    \"\"\"\n",
    "\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 25°C\",\n",
    "        \"Los Angeles\": \"Cloudy, 22°C\",\n",
    "        \"Chicago\": \"Rainy, 18°C\"\n",
    "    }\n",
    "    return weather_data.get(location, \"Weather data not available for this location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a1a5d",
   "metadata": {},
   "source": [
    "### Step 3: Bind Tools to the LLM\n",
    "\n",
    "`bind_tools()` tells the LLM about available functions. The LLM now knows:\n",
    "- What tools are available\n",
    "- What parameters each tool expects  \n",
    "- When each tool should be used\n",
    "\n",
    "This creates a new LLM instance that's \"tool-aware\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ae68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([weather_tool])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa0b59",
   "metadata": {},
   "source": [
    "### Step 4: Create the Initial Conversation\n",
    "\n",
    "We set up a conversation asking about weather information. Notice the user asks about \"NYC\" - the LLM will need to map this to \"New York\" for our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3ede1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that can provide weather information for specific cities.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is the weather like in NYC?\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df18b2",
   "metadata": {},
   "source": [
    "### Step 5: First LLM Call - Tool Selection\n",
    "\n",
    "The LLM analyzes the request and determines that weather information is needed. Instead of generating a text response, it returns:\n",
    "- **Tool call instructions** with the function name and parameters\n",
    "- **A unique call ID** to track this specific tool invocation\n",
    "\n",
    "We add this response to our message history to maintain context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f867a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke(input=messages)\n",
    "#Adding the response to the prompt history\n",
    "messages.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813db24",
   "metadata": {},
   "source": [
    "### Step 6: Execute Tools and Second LLM Call\n",
    "\n",
    "Here's where the magic happens:\n",
    "\n",
    "1. **Check for tool calls**: If the LLM requested tools, we process them\n",
    "2. **Execute each tool**: We run the actual function with the LLM's parameters\n",
    "3. **Create tool messages**: Package the results in ToolMessage objects\n",
    "4. **Add to conversation**: Include tool results in the message history\n",
    "5. **Second LLM call**: The LLM formulates a final response using the tool output\n",
    "\n",
    "The `tool_call_id` links the tool result back to the original request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08770b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(response.tool_calls) > 0:\n",
    "    for tool_call in response.tool_calls:\n",
    "        messages.append(\n",
    "            # Adding the tool call to the prompt history\n",
    "            ToolMessage(\n",
    "                name=tool_call[\"name\"],\n",
    "                content=weather_tool.run(tool_call[\"args\"][\"location\"]), #Executing the tool\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Calling the LLM again with the updated messages, containing the tool response\n",
    "    response = llm_with_tools.invoke(input=messages)\n",
    "    # Adding the final response to the prompt history\n",
    "    messages.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa6392",
   "metadata": {},
   "source": [
    "### Step 7: View the Complete Conversation Flow\n",
    "\n",
    "The output shows the complete conversation including:\n",
    "- Original system and human messages\n",
    "- AI message with tool call instructions\n",
    "- Tool message with the actual result\n",
    "- Final AI message with the formatted response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0946d022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant that can provide weather information for specific cities.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather like in NYC?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  weather_tool (call_4d03xCDEnFotRMjeg8w5UP4j)\n",
      " Call ID: call_4d03xCDEnFotRMjeg8w5UP4j\n",
      "  Args:\n",
      "    location: New York\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: weather_tool\n",
      "\n",
      "Sunny, 25°C\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in New York City is currently sunny with a temperature of 25°C.\n"
     ]
    }
   ],
   "source": [
    "for msg in messages:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b659455",
   "metadata": {},
   "source": [
    "## Understanding the Tool Call Flow\n",
    "\n",
    "Notice the Tool Calls and Tool Messages in the conversation flow above. This demonstrates the complete cycle:\n",
    "\n",
    "1. **Human Message**: \"What is the weather like in NYC?\"\n",
    "2. **AI Message with Tool Call**: LLM decides to use `weather_tool` with parameter `location: \"New York\"`\n",
    "3. **Tool Message**: Actual execution result: \"Sunny, 25°C\"\n",
    "4. **Final AI Message**: Formatted response incorporating the tool output\n",
    "\n",
    "**Important**: Tool usage means **2 LLM calls** minimum:\n",
    "- **First call**: Determine what tool is necessary\n",
    "- **Second call**: Generate response containing the tool result\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Tool Binding**: Use `bind_tools()` to make functions available to the LLM\n",
    "2. **Tool Detection**: Check `response.tool_calls` to see if tools are needed\n",
    "3. **Tool Execution**: Your application executes tools, not the LLM\n",
    "4. **Result Integration**: Tool outputs become part of the conversation context\n",
    "5. **Cost Consideration**: Each tool use requires multiple LLM calls\n",
    "\n",
    "## When to Use Tool Calling\n",
    "\n",
    "- **Real-time Data**: Weather, stock prices, current events\n",
    "- **External APIs**: Database queries, web searches, social media\n",
    "- **Calculations**: Math operations, data processing\n",
    "- **File Operations**: Reading, writing, or analyzing files\n",
    "- **System Actions**: Sending emails, creating calendar events\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "This manual tool calling approach works for simple cases, but becomes complex with:\n",
    "- Multiple tools\n",
    "- Conditional tool sequences\n",
    "- Error handling and retries\n",
    "\n",
    "The next notebook will show **ReAct Agents** that automate this tool calling process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
