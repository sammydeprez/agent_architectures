{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ab4402",
   "metadata": {},
   "source": [
    "# 4. Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d05f16",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The Reflection pattern introduces self-evaluation capabilities to agent systems. This architecture enables agents to:\n",
    "\n",
    "- **Analyze their own outputs** for quality and completeness\n",
    "- **Make iterative improvements** by regenerating responses\n",
    "- **Ensure requirements are met** before finalizing responses\n",
    "- **Implement quality control** through automated review cycles\n",
    "\n",
    "![Reflection Architecture](../docs/reflection.png)\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Generator Agent**: Creates initial responses to user queries\n",
    "2. **Reflection Agent**: Evaluates the quality and completeness of responses\n",
    "3. **Feedback Loop**: Allows for regeneration based on reflection results\n",
    "4. **Quality Criteria**: Structured evaluation framework\n",
    "\n",
    "This pattern is particularly useful for:\n",
    "- **Complex reasoning tasks** requiring multiple iterations\n",
    "- **Content creation** with quality standards\n",
    "- **Customer service** where accuracy is critical\n",
    "- **Educational applications** with learning feedback\n",
    "\n",
    "## Implementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Dependencies\n",
    "\n",
    "We start by importing the necessary libraries for implementing the reflection pattern:\n",
    "- **LangChain OpenAI**: For LLM integration\n",
    "- **Message types**: For structured conversation handling\n",
    "- **LangGraph**: For building the reflection workflow with conditional routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa434df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from typing import Literal\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph import END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the Language Model\n",
    "\n",
    "We configure the Azure OpenAI model that will serve both as the **generator** and **reflection agent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "### Step 3: Create the Generator Agent\n",
    "\n",
    "The **travel_advice** function acts as our content generator. It:\n",
    "- Receives the conversation state\n",
    "- Generates travel advice based on user input\n",
    "- Returns the response to be evaluated by the reflection agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6276a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def travel_advice(state) -> str:\n",
    "    print(\"Node: Travel Advice\")\n",
    "    response = llm.invoke(input=state[\"messages\"])\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "### Step 4: Define Reflection Output Structure\n",
    "\n",
    "We use Pydantic to define the structured output format for our reflection agent. This ensures:\n",
    "- **Consistent evaluation criteria**: Each topic is checked systematically\n",
    "- **Boolean feedback**: Clear yes/no answers for content inclusion\n",
    "- **Topic tracking**: Identifies specific areas that need improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a234371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class ReflectionOutput(BaseModel):\n",
    "    \"\"\"You reflect on the conversation and checks if input contains the requested topic.\"\"\"\n",
    "    topic: str\n",
    "    contains_info: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "### Step 5: Implement the Reflection Agent\n",
    "\n",
    "The **reflection** function is the heart of our quality control system. It:\n",
    "\n",
    "1. **Analyzes the generated content** against predefined quality criteria\n",
    "2. **Checks for completeness** across multiple topics (sports, cultural, historical)\n",
    "3. **Makes routing decisions**:\n",
    "   - **Continue to generator**: If content is missing required topics\n",
    "   - **End workflow**: If all criteria are met\n",
    "4. **Provides feedback**: Specific instructions for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection(state) -> Command[Literal[\"travel_advice\", \"__end__\"]]:\n",
    "    print(\"Node: Reflection\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    structured_llm = llm.with_structured_output(ReflectionOutput)\n",
    "\n",
    "    topic_check_list = [\"sports\", \"cultural\", \"historical\"]\n",
    "    validations = []\n",
    "    for topic in topic_check_list:\n",
    "        response = structured_llm.invoke(input=[\n",
    "            SystemMessage(content=f\"You are a content expert, and validate if a messages contains information about topic \\\"{topic}\\\"?\"),\n",
    "            last_message\n",
    "        ])\n",
    "        validations.append(response)\n",
    "\n",
    "    missing_topics = [response for response in validations if not response.contains_info]\n",
    "    print(f\"Missing topics: {[response.topic for response in missing_topics]}\")\n",
    "\n",
    "    if len(missing_topics) > 0:\n",
    "        return Command(\n",
    "            goto=\"travel_advice\",\n",
    "            update={\"messages\": [AIMessage(content=\"Extend output with information about following topics: \" \n",
    "                                           + \", \".join([response.topic for response in missing_topics]))]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=END\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i9",
   "metadata": {},
   "source": [
    "### Step 6: Build the Reflection Workflow\n",
    "\n",
    "We construct a StateGraph that defines the reflection pattern workflow:\n",
    "- **Nodes**: travel_advice (generator) and reflection (evaluator)\n",
    "- **Edges**: Define the flow between generation and evaluation\n",
    "- **Conditional routing**: Reflection agent determines next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "builder.add_node(\"travel_advice\", travel_advice)\n",
    "builder.add_node(\"reflection\", reflection)\n",
    "\n",
    "builder.add_edge(START, \"travel_advice\")\n",
    "builder.add_edge(\"travel_advice\", \"reflection\")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "### Step 7: Visualize the Workflow\n",
    "\n",
    "Generate a visual representation of our reflection pattern to understand the flow between components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6948ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k1",
   "metadata": {},
   "source": [
    "### Step 8: Test the Reflection Pattern\n",
    "\n",
    "Let's test our reflection system with a simple travel query. The system will:\n",
    "1. Generate initial travel advice\n",
    "2. Evaluate content completeness\n",
    "3. Request improvements if needed\n",
    "4. Iterate until all criteria are met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that gives travel advice based on the user's input.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Perth\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "### Step 9: Execute and Monitor the Reflection Loop\n",
    "\n",
    "Run the reflection workflow and observe how the system:\n",
    "- **Generates content**\n",
    "- **Evaluates quality**\n",
    "- **Provides feedback**\n",
    "- **Iteratively improves**\n",
    "\n",
    "Watch for the feedback loop in action as the system refines its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in messages:\n",
    "    msg.pretty_print()\n",
    "\n",
    "async for event in graph.astream(input={\"messages\": messages}, stream_mode=\"updates\"):\n",
    "    for node in graph.nodes.keys():\n",
    "        node_output = event.get(node, {})\n",
    "        if node_output is not None:\n",
    "            output_msgs = node_output.get(\"messages\", [])\n",
    "            for msg in output_msgs:\n",
    "                msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m3",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Benefits of the Reflection Pattern\n",
    "- **Quality Assurance**: Automated content evaluation ensures consistent standards\n",
    "- **Iterative Improvement**: Continuous refinement until criteria are met\n",
    "- **Structured Feedback**: Clear, actionable improvement suggestions\n",
    "- **Scalable Evaluation**: Easily extend evaluation criteria for different domains\n",
    "\n",
    "### When to Use Reflection\n",
    "- **Content Creation**: Blog posts, documentation, reports requiring completeness\n",
    "- **Customer Support**: Ensuring responses address all customer concerns\n",
    "- **Educational Content**: Verifying learning objectives are covered\n",
    "- **Quality Control**: Any scenario where output standards matter\n",
    "\n",
    "### Performance Considerations\n",
    "- **Cost**: Multiple LLM calls increase operational costs\n",
    "- **Latency**: Reflection loops add processing time\n",
    "- **Stopping Criteria**: Set maximum iterations to prevent infinite loops\n",
    "\n",
    "### Next Steps\n",
    "Consider combining reflection with other patterns:\n",
    "- **Routing + Reflection**: Route to specialized agents with quality control\n",
    "- **Tool Calling + Reflection**: Verify tool usage and results\n",
    "- **Parallelism + Reflection**: Concurrent generation with centralized evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f5da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
