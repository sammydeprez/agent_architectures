{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d518d075",
   "metadata": {},
   "source": [
    "# 1. Simple LLM Call with Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae09436",
   "metadata": {},
   "source": [
    "An LLM call is the simplest agent that is there. It can not execute any actions, it only returns tokens that fit to your prompt. This example shows how to make a basic LLM call using Semantic Kernel instead of LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478aad6",
   "metadata": {},
   "source": [
    "Create the kernel and configure Azure OpenAI service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f16949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kernel instance\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion service\n",
    "# Note: This assumes you have Azure OpenAI credentials configured via environment variables\n",
    "# AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME\n",
    "service_id = \"azure_openai\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        deployment_name=\"gpt-4.1-mini\",  # Your Azure OpenAI deployment name\n",
    "        # api_key and endpoint will be read from environment variables\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19416f7",
   "metadata": {},
   "source": [
    "Create chat history and add system and user messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ceb23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat history\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "# Add system message\n",
    "chat_history.add_system_message(\"You are a travel expert. Based on the user's input, suggest 1 travel destination.\")\n",
    "\n",
    "# Add user message\n",
    "chat_history.add_user_message(\"Cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a3b13",
   "metadata": {},
   "source": [
    "Call the LLM and get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66240557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the chat completion service\n",
    "chat_completion = kernel.get_service(service_id)\n",
    "\n",
    "# Generate response\n",
    "response = await chat_completion.get_chat_message_contents(\n",
    "    chat_history=chat_history,\n",
    "    settings=kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    ")\n",
    "\n",
    "# Add the response to chat history\n",
    "chat_history.add_message(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b7559",
   "metadata": {},
   "source": [
    "Print the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16921fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the conversation\n",
    "for message in chat_history.messages:\n",
    "    role = message.role.value.title()\n",
    "    content = str(message.content)\n",
    "    print(f\"================================ {role} Message ================================\")\n",
    "    print()\n",
    "    print(content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_differences",
   "metadata": {},
   "source": [
    "## Key Differences from LangChain\n",
    "\n",
    "1. **Kernel-based Architecture**: Semantic Kernel uses a `Kernel` object as the central orchestrator, where you register services and manage the conversation.\n",
    "\n",
    "2. **Service Registration**: Instead of directly instantiating the LLM, you add it as a service to the kernel with a service ID.\n",
    "\n",
    "3. **ChatHistory**: Semantic Kernel provides a dedicated `ChatHistory` class to manage conversation state, with specific methods for different message types.\n",
    "\n",
    "4. **Async by Design**: The chat completion calls are async by default in Semantic Kernel.\n",
    "\n",
    "5. **Service-oriented**: You retrieve services from the kernel by their ID rather than calling them directly.\n",
    "\n",
    "This approach provides better separation of concerns and makes it easier to swap different AI services or add multiple services to the same kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}