{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea669370",
   "metadata": {},
   "source": [
    "# 2.1 Using Tools with Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f6edd",
   "metadata": {},
   "source": [
    "By adding functions (tools) to your kernel, the model gets information about those functions, and it will tell you that it wants to execute a function. The kernel can automatically handle function execution and return results back to the LLM. This is similar to LangChain's tool binding but uses Semantic Kernel's plugin system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa434df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from typing import Annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621924ca",
   "metadata": {},
   "source": [
    "Create the kernel and configure Azure OpenAI service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kernel instance\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion service\n",
    "service_id = \"azure_openai\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        deployment_name=\"gpt-4.1-mini\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0c4f0",
   "metadata": {},
   "source": [
    "Define the weather tool as a Semantic Kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherPlugin:\n",
    "    \"\"\"\n",
    "    A plugin that provides weather information for specific cities.\n",
    "    \"\"\"\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Get the current weather for a specified location\",\n",
    "        name=\"get_weather\"\n",
    "    )\n",
    "    def get_weather(\n",
    "        self,\n",
    "        location: Annotated[str, \"The name of the city to get the weather for. Must be one of 'Chicago', 'New York', or 'Los Angeles'\"]\n",
    "    ) -> Annotated[str, \"A string describing the current weather in the specified location\"]:\n",
    "        \"\"\"\n",
    "        Get the current weather for a specified location.\n",
    "        \"\"\"\n",
    "        weather_data = {\n",
    "            \"New York\": \"Sunny, 25°C\",\n",
    "            \"Los Angeles\": \"Cloudy, 22°C\",\n",
    "            \"Chicago\": \"Rainy, 18°C\"\n",
    "        }\n",
    "        return weather_data.get(location, \"Weather data not available for this location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a1a5d",
   "metadata": {},
   "source": [
    "Add the plugin to the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the weather plugin to the kernel\n",
    "kernel.add_plugin(WeatherPlugin(), plugin_name=\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa0b59",
   "metadata": {},
   "source": [
    "Set up chat history with system and user messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ede1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat history\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "# Add system and user messages\n",
    "chat_history.add_system_message(\n",
    "    \"You are a helpful assistant that can provide weather information for specific cities.\"\n",
    ")\n",
    "chat_history.add_user_message(\"What is the weather like in NYC?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df18b2",
   "metadata": {},
   "source": [
    "Configure function calling and get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f867a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure execution settings to enable automatic function calling\n",
    "execution_settings = kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Get the chat completion service\n",
    "chat_completion = kernel.get_service(service_id)\n",
    "\n",
    "# Generate response with automatic function calling\n",
    "response = await chat_completion.get_chat_message_contents(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel  # Pass kernel to enable function calling\n",
    ")\n",
    "\n",
    "# Add the response to chat history\n",
    "chat_history.add_message(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa6392",
   "metadata": {},
   "source": [
    "Print the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the conversation\n",
    "for message in chat_history.messages:\n",
    "    role = message.role.value.title()\n",
    "    content = str(message.content)\n",
    "    print(f\"================================ {role} Message ================================\")\n",
    "    print()\n",
    "    print(content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b659455",
   "metadata": {},
   "source": [
    "Notice how the function call and execution happens automatically when `FunctionChoiceBehavior.Auto()` is used. The kernel handles:\n",
    "1. Function discovery and schema generation\n",
    "2. Function calling by the LLM\n",
    "3. Function execution\n",
    "4. Result integration back into the conversation\n",
    "\n",
    "**NOTE:** Usage of functions means multiple LLM calls may happen automatically - one to determine what function to call and potentially more to process the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_differences",
   "metadata": {},
   "source": [
    "## Key Differences from LangChain\n",
    "\n",
    "1. **Plugin System**: Semantic Kernel uses a plugin-based architecture where functions are organized into plugins (classes) rather than individual tool decorators.\n",
    "\n",
    "2. **Kernel Functions**: Use `@kernel_function` decorator instead of `@tool` to define functions available to the LLM.\n",
    "\n",
    "3. **Automatic Function Calling**: With `FunctionChoiceBehavior.Auto()`, the kernel automatically handles function calling, execution, and result integration.\n",
    "\n",
    "4. **Type Annotations**: Semantic Kernel leverages Python's `Annotated` type hints for better function schema generation and documentation.\n",
    "\n",
    "5. **Simplified Flow**: No need to manually check for tool calls or handle tool messages - the kernel manages the entire flow automatically.\n",
    "\n",
    "6. **Plugin Organization**: Functions are grouped into logical plugins, making it easier to manage and organize capabilities.\n",
    "\n",
    "This approach provides better abstraction and automation compared to manually handling tool calls in LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}