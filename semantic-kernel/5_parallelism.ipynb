{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77aa15d",
   "metadata": {},
   "source": [
    "# 5. Parallelism with Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallelism_intro",
   "metadata": {},
   "source": [
    "The parallelism pattern involves processing multiple similar tasks concurrently to improve efficiency. In this example, we'll generate random cities and then process each one in parallel to get interesting facts about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d579b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "import asyncio\n",
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kernel_setup",
   "metadata": {},
   "source": [
    "Set up the kernel and service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5079b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kernel instance\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion service\n",
    "service_id = \"azure_openai\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        deployment_name=\"gpt-4.1-mini\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agents_creation",
   "metadata": {},
   "source": [
    "Create specialized agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# City Generator Agent\n",
    "city_generator = ChatCompletionAgent(\n",
    "    service_id=service_id,\n",
    "    kernel=kernel,\n",
    "    name=\"CityGenerator\",\n",
    "    instructions=\"Generate random city names. When asked, provide exactly 3 city names from the specified country, separated by commas. Only return the city names, no other text.\",\n",
    "    execution_settings={\n",
    "        service_id: kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    "    }\n",
    ")\n",
    "\n",
    "# City Facts Agent\n",
    "city_facts_agent = ChatCompletionAgent(\n",
    "    service_id=service_id,\n",
    "    kernel=kernel,\n",
    "    name=\"CityFactsAgent\",\n",
    "    instructions=\"\"\"Provide exactly 3 interesting facts about the given city.\n",
    "    \n",
    "Format your response as:\n",
    "# CityName\n",
    "1. First interesting fact\n",
    "2. Second interesting fact  \n",
    "3. Third interesting fact\n",
    "\n",
    "Only return the facts, no other text. Use a numbered list.\"\"\",\n",
    "    execution_settings={\n",
    "        service_id: kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_functions",
   "metadata": {},
   "source": [
    "Create helper functions for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_random_cities(country: str = \"India\", count: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate random city names from the specified country\n",
    "    \"\"\"\n",
    "    print(f\"→ Generating {count} random cities from {country}...\")\n",
    "    \n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_user_message(f\"Return {count} random city names in {country}, comma separated.\")\n",
    "    \n",
    "    async for response in city_generator.invoke(chat_history):\n",
    "        cities_text = str(response.content)\n",
    "        # Parse the comma-separated cities\n",
    "        cities = [city.strip() for city in cities_text.split(',')]\n",
    "        print(f\"Generated cities: {cities}\")\n",
    "        return cities\n",
    "    \n",
    "    return []\n",
    "\n",
    "async def get_city_facts(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Get interesting facts about a specific city\n",
    "    \"\"\"\n",
    "    print(f\"→ Getting facts for {city}...\")\n",
    "    \n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_user_message(f\"Tell me 3 interesting facts about {city}. Only return the facts, no other text. Use a numbered list. Start with the city name. Ex. # CityName\")\n",
    "    \n",
    "    async for response in city_facts_agent.invoke(chat_history):\n",
    "        facts = str(response.content)\n",
    "        print(f\"✓ Completed facts for {city}\")\n",
    "        return facts\n",
    "    \n",
    "    return f\"No facts available for {city}\"\n",
    "\n",
    "async def process_cities_parallel(cities: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process multiple cities in parallel to get facts about each\n",
    "    \"\"\"\n",
    "    print(f\"\\n→ Processing {len(cities)} cities in parallel...\")\n",
    "    \n",
    "    # Create tasks for parallel execution\n",
    "    tasks = [get_city_facts(city) for city in cities]\n",
    "    \n",
    "    # Execute all tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(f\"✓ Completed processing all {len(cities)} cities\\n\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_workflow",
   "metadata": {},
   "source": [
    "Create the main parallel workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_workflow_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parallel_city_facts_workflow(country: str = \"India\", city_count: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Main workflow that demonstrates parallel processing\n",
    "    \"\"\"\n",
    "    print(f\"=== Parallel City Facts Workflow ===\")\n",
    "    print(f\"Country: {country}, Cities to process: {city_count}\\n\")\n",
    "    \n",
    "    # Step 1: Generate random cities\n",
    "    cities = await generate_random_cities(country, city_count)\n",
    "    \n",
    "    if not cities:\n",
    "        print(\"❌ No cities generated\")\n",
    "        return {\"cities\": [], \"facts\": []}\n",
    "    \n",
    "    # Step 2: Process cities in parallel\n",
    "    facts = await process_cities_parallel(cities)\n",
    "    \n",
    "    # Step 3: Return results\n",
    "    return {\n",
    "        \"cities\": cities,\n",
    "        \"facts\": facts\n",
    "    }\n",
    "\n",
    "def display_results(results: dict):\n",
    "    \"\"\"\n",
    "    Display the results in a formatted way\n",
    "    \"\"\"\n",
    "    print(\"=== RESULTS ===\")\n",
    "    print(f\"Cities processed: {results['cities']}\")\n",
    "    print(f\"Total facts collected: {len(results['facts'])}\\n\")\n",
    "    \n",
    "    print(\"=== CITY FACTS ===\")\n",
    "    for fact in results['facts']:\n",
    "        print(fact)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_basic_parallel",
   "metadata": {},
   "source": [
    "Test the basic parallel workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Indian cities\n",
    "results = await parallel_city_facts_workflow(\"India\", 3)\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_different_country",
   "metadata": {},
   "source": [
    "Test with a different country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Japanese cities\n",
    "results_japan = await parallel_city_facts_workflow(\"Japan\", 4)\n",
    "display_results(results_japan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_comparison",
   "metadata": {},
   "source": [
    "## Performance Comparison: Sequential vs Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "async def sequential_processing(cities: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process cities sequentially for comparison\n",
    "    \"\"\"\n",
    "    print(f\"→ Processing {len(cities)} cities sequentially...\")\n",
    "    results = []\n",
    "    \n",
    "    for city in cities:\n",
    "        fact = await get_city_facts(city)\n",
    "        results.append(fact)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Performance comparison\n",
    "test_cities = [\"Tokyo\", \"Osaka\", \"Kyoto\", \"Hiroshima\"]\n",
    "\n",
    "# Test parallel processing\n",
    "print(\"Testing PARALLEL processing:\")\n",
    "start_time = time.time()\n",
    "parallel_results = await process_cities_parallel(test_cities)\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTesting SEQUENTIAL processing:\")\n",
    "start_time = time.time()\n",
    "sequential_results = await sequential_processing(test_cities)\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "print(f\"Parallel processing time: {parallel_time:.2f} seconds\")\n",
    "print(f\"Sequential processing time: {sequential_time:.2f} seconds\")\n",
    "print(f\"Speed improvement: {sequential_time/parallel_time:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch_processing",
   "metadata": {},
   "source": [
    "## Advanced: Batch Processing with Concurrency Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch_processing_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def batch_process_cities(cities: List[str], batch_size: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process cities in batches to control concurrency\n",
    "    \"\"\"\n",
    "    print(f\"→ Processing {len(cities)} cities in batches of {batch_size}...\")\n",
    "    all_results = []\n",
    "    \n",
    "    for i in range(0, len(cities), batch_size):\n",
    "        batch = cities[i:i + batch_size]\n",
    "        print(f\"  Processing batch {i//batch_size + 1}: {batch}\")\n",
    "        \n",
    "        batch_results = await process_cities_parallel(batch)\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Test batch processing with a larger list\n",
    "large_city_list = [\"Mumbai\", \"Delhi\", \"Bangalore\", \"Chennai\", \"Kolkata\", \"Hyderabad\", \"Pune\", \"Ahmedabad\"]\n",
    "batch_results = await batch_process_cities(large_city_list, batch_size=3)\n",
    "\n",
    "print(f\"\\n=== BATCH PROCESSING RESULTS ===\")\n",
    "print(f\"Total cities processed: {len(large_city_list)}\")\n",
    "print(f\"Results collected: {len(batch_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_differences",
   "metadata": {},
   "source": [
    "## Key Differences from LangChain Parallelism\n",
    "\n",
    "1. **Asyncio-Based Concurrency**: Uses Python's native `asyncio.gather()` for parallel execution instead of complex graph structures with `Send` commands.\n",
    "\n",
    "2. **Simplified Coordination**: No need for complex state management or command objects - just straightforward async functions.\n",
    "\n",
    "3. **Agent-Based Architecture**: Each processing unit is a dedicated `ChatCompletionAgent` with specific instructions.\n",
    "\n",
    "4. **Flexible Batching**: Easy to implement batch processing to control concurrency and avoid overwhelming the AI service.\n",
    "\n",
    "5. **Better Error Handling**: Standard Python exception handling works naturally with async/await patterns.\n",
    "\n",
    "6. **Performance Monitoring**: Easy to measure and compare performance between parallel and sequential approaches.\n",
    "\n",
    "7. **Scalable Design**: Can easily adjust concurrency levels and batch sizes based on requirements.\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- **Simplicity**: Uses standard Python async patterns that developers are familiar with\n",
    "- **Performance**: Achieves significant speed improvements for concurrent tasks\n",
    "- **Flexibility**: Easy to adjust concurrency levels and processing strategies\n",
    "- **Reliability**: Better error handling and recovery options\n",
    "- **Maintainability**: Clear, readable code that's easy to debug and modify\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Bulk Data Processing**: Processing multiple similar items concurrently\n",
    "- **Content Generation**: Creating content for multiple topics simultaneously\n",
    "- **Data Enrichment**: Enhancing datasets with AI-generated information\n",
    "- **Multi-Source Analysis**: Analyzing multiple data sources in parallel\n",
    "- **Batch Operations**: Processing large lists of items efficiently\n",
    "\n",
    "This approach provides a clean, efficient way to implement parallelism without the complexity of graph-based workflows, making it easier to understand, maintain, and scale.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}