{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eccebb4",
   "metadata": {},
   "source": [
    "# 2.2 Chat Completion Agent with Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22d7ac",
   "metadata": {},
   "source": [
    "A Chat Completion Agent in Semantic Kernel provides a streamlined way to handle function calling with automatic execution. This is equivalent to LangChain's ReAct Agent but uses Semantic Kernel's ChatCompletionAgent, which automatically handles the conversation flow and function execution behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa434df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from typing import Annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298acde4",
   "metadata": {},
   "source": [
    "Create the kernel and configure Azure OpenAI service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kernel instance\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion service\n",
    "service_id = \"azure_openai\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        deployment_name=\"gpt-4.1-mini\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ce9b1",
   "metadata": {},
   "source": [
    "Define the weather tool as a Semantic Kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherPlugin:\n",
    "    \"\"\"\n",
    "    A plugin that provides weather information for specific cities.\n",
    "    \"\"\"\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Get the current weather for a specified location\",\n",
    "        name=\"get_weather\"\n",
    "    )\n",
    "    def get_weather(\n",
    "        self,\n",
    "        location: Annotated[str, \"The name of the city to get the weather for. Must be one of 'Chicago', 'New York', or 'Los Angeles'\"]\n",
    "    ) -> Annotated[str, \"A string describing the current weather in the specified location\"]:\n",
    "        \"\"\"\n",
    "        Get the current weather for a specified location.\n",
    "        \"\"\"\n",
    "        weather_data = {\n",
    "            \"New York\": \"Sunny, 25°C\",\n",
    "            \"Los Angeles\": \"Cloudy, 22°C\",\n",
    "            \"Chicago\": \"Rainy, 18°C\"\n",
    "        }\n",
    "        return weather_data.get(location, \"Weather data not available for this location.\")\n",
    "\n",
    "# Add the weather plugin to the kernel\n",
    "kernel.add_plugin(WeatherPlugin(), plugin_name=\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aeb504",
   "metadata": {},
   "source": [
    "Create the Chat Completion Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatCompletionAgent with function calling enabled\n",
    "agent = ChatCompletionAgent(\n",
    "    service_id=service_id,\n",
    "    kernel=kernel,\n",
    "    name=\"WeatherAgent\",\n",
    "    instructions=\"You are a helpful assistant that can provide weather information for specific cities.\",\n",
    "    execution_settings={\n",
    "        service_id: kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Enable automatic function calling\n",
    "agent.execution_settings[service_id].function_choice_behavior = FunctionChoiceBehavior.Auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58205e",
   "metadata": {},
   "source": [
    "Semantic Kernel agents provide a more structured approach to conversation management. Let's visualize the agent's capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display agent information\n",
    "print(f\"Agent Name: {agent.name}\")\n",
    "print(f\"Agent Instructions: {agent.instructions}\")\n",
    "print(f\"Available Functions:\")\n",
    "for plugin_name in kernel.plugins:\n",
    "    plugin = kernel.plugins[plugin_name]\n",
    "    for function_name in plugin:\n",
    "        function = plugin[function_name]\n",
    "        print(f\"  - {plugin_name}.{function_name}: {function.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b4592",
   "metadata": {},
   "source": [
    "Create chat history and interact with the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat history\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "# Add user message\n",
    "chat_history.add_user_message(\"What is the weather like in NYC?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c280c",
   "metadata": {},
   "source": [
    "Have the agent respond to the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get agent response\n",
    "async for response in agent.invoke(chat_history):\n",
    "    chat_history.add_message(response)\n",
    "    break  # Get the first (and likely only) response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab50f09",
   "metadata": {},
   "source": [
    "Print the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08770b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the conversation\n",
    "for message in chat_history.messages:\n",
    "    role = message.role.value.title()\n",
    "    content = str(message.content)\n",
    "    print(f\"================================ {role} Message ================================\")\n",
    "    print()\n",
    "    print(content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_multi_turn",
   "metadata": {},
   "source": [
    "## Multi-turn Conversation Example\n",
    "\n",
    "Let's demonstrate how the agent maintains context across multiple turns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi_turn_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "chat_history.add_user_message(\"What about Chicago?\")\n",
    "\n",
    "# Get agent response\n",
    "async for response in agent.invoke(chat_history):\n",
    "    chat_history.add_message(response)\n",
    "    break\n",
    "\n",
    "# Add another question\n",
    "chat_history.add_user_message(\"Which city has the warmest weather?\")\n",
    "\n",
    "# Get final response\n",
    "async for response in agent.invoke(chat_history):\n",
    "    chat_history.add_message(response)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print_full_conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the full conversation\n",
    "print(\"=== Full Conversation ===\")\n",
    "for i, message in enumerate(chat_history.messages, 1):\n",
    "    role = message.role.value.title()\n",
    "    content = str(message.content)\n",
    "    print(f\"[{i}] {role}: {content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_differences",
   "metadata": {},
   "source": [
    "## Key Differences from LangChain ReAct Agent\n",
    "\n",
    "1. **Agent-First Design**: Semantic Kernel's `ChatCompletionAgent` is designed as a first-class citizen that encapsulates instructions, execution settings, and behavior.\n",
    "\n",
    "2. **Automatic Flow Management**: The agent automatically handles the conversation flow, function calling, and response generation without manual intervention.\n",
    "\n",
    "3. **Streaming Support**: The `invoke` method returns an async generator, allowing for streaming responses and better control over conversation flow.\n",
    "\n",
    "4. **Built-in Context Management**: The agent maintains conversation context automatically and can handle multi-turn conversations seamlessly.\n",
    "\n",
    "5. **Execution Settings Integration**: Agent-specific execution settings are cleanly integrated, making it easy to configure behavior per agent.\n",
    "\n",
    "6. **Plugin Integration**: The agent automatically discovers and can use any functions from plugins added to the kernel.\n",
    "\n",
    "7. **Simplified API**: Compared to LangChain's ReAct agent setup, Semantic Kernel provides a more straightforward API for creating and using agents.\n",
    "\n",
    "This approach provides better encapsulation and makes it easier to build complex multi-agent systems where each agent has specific roles and capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}